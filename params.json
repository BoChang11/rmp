{
  "name": "Motion Planning - Reeds-Shepp Car",
  "tagline": "By Sampling Based Methods(RRT & RRT*)",
  "body": "# Path Planning for Reeds-Shepp Car\r\n\r\n## Table of Contents\r\n - [Introduction](#introduction)\r\n - [Model Description](#model-description)\r\n - [Limitations](#limitations)\r\n - [Collision Detection](#collision-detection)\r\n - [Path Planning Techniques](#path-planning-techniques)\r\n    - [RRT](#rrt)\r\n    - [RRT\\*](#rrt-star)\r\n    - [Type 1 and Type 2 maneuvers](#maneuvers)\r\n - [Results](#results)\r\n    - [Holonomic Robot](#holonomic-robot)\r\n    - [Non-Holonomic Robot](#non-holonomic-robot)\r\n - [Execution Instruction](#execution-instruction)\r\n    - [Dependencies](#dependencies)\r\n    - [Creating and Loading Scene](#creating-and-loading-scene)\r\n    - [Running Path Planning Methods](#running-path-planning-methods)\r\n - [Source Code](#source-code)\r\n - [Future Work](#future-work)\r\n - [References](#references)\r\n \r\n## Introduction\r\nThe goal of this project is to implement sampling based motion planning \r\nalgorithms such as Rapidly-Exploring Random Tree(RRT) and its variant RRT* for a \r\nholonomic point robot and a Reeds-Shepp car like robot with \r\nnon-holonomic constraints.\r\n\r\nWhen I was implementing RRT* with non-holonomic constraints, I\r\nnoticed that we need to be able to reach a nearby point(q_near) with its \r\norientation from the new configuration(q_new). This is mostly impossible to reach\r\nby just trying the six combination of inputs that a Reeds-Shepp Car can\r\ntake. So I also implemented ***Type 1*** and ***Type 2*** maneuvers for a car like \r\nrobot to reach a desired configuration by doing combinations of these two\r\nmaneuvers.\r\n \r\n \r\n## Model Description\r\n - Robot Model &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:   Point Robot and Reeds-Shepp Car(Rigid Body)\r\n - Type        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:   Holonomic and Non-Holonomic\r\n - Workspace   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:   2-D plane with Obstacles\r\n - Collision Detection: Axis Aligned Bounding Boxes(Implemented)\r\n\r\n## Limitations\r\nFor the non-holonomic case, I'm still defining the robot as point\r\nbecause of my collision detection methods can only tell whether a point\r\nlies inside the obstacle or not. I didn't implement polygon-polygon \r\ncollision detection methods to facilitate this and also using\r\nexisting libraries for collision detection even complicates my data structures.\r\n \r\n## Collision Detection\r\n### Axis Aligned Bounding Box\r\nThe collision detection method that I implemented and used to check\r\n collision for the below path planning techniques is Axis aligned \r\n bounding boxes. Here every obstacle in the environment is bounded by a \r\n rectangle of smallest possible size and checking the robot is inside the\r\n rectangle while planning the path.\r\n \r\n## Path Planning Techniques\r\n - RRT\r\n - RRT*\r\n - Type 1 and Type 2 Maneuvers\r\n \r\n### Type 1 and Type 2 Maneuvers\r\nType 1: This maneuver is to take the robot from a configuration to another\r\nconfiguration without changing the orientation(Like parallel parking). \r\n\r\n![Type 1](https://raw.githubusercontent.com/iamprem/temp/master/assets/type_1.png)\r\n\r\n\r\nType 2: This maneuver is to bring a robot from some orientation to a\r\ndesired orientation in-place(Like a three-point turn)\r\n\r\n![Type 2](https://raw.githubusercontent.com/iamprem/temp/master/assets/type_2.png)\r\n\r\n***Images Source: Class Lectures***\r\n\r\nThere was a third type of movement is implemented to connect these two\r\nmaneuvers to reach the goal exactly. First, type 2 maneuver is done to \r\nchange to desired orientation and then marching forward or backward till it \r\nfinds a point that is exactly parallel to the goal configuration, then \r\ntype 1 maneuver is done to reach the goal configuration. Below GIF shows\r\ntype 1, type 2 and marching in cyan, brown and yellow colors respectively\r\nfrom the simulation.\r\n\r\n![Maneuvers](https://raw.githubusercontent.com/iamprem/temp/master/assets/maneuver_gif.gif)\r\n\r\n## Results\r\n\r\n### Holonomic Robot\r\n\r\n#### RRT with 10000 random samples \r\n\r\n![RRT Holonomic](https://raw.githubusercontent.com/iamprem/temp/master/assets/rrt_holo.gif)\r\n\r\n#### RRT* with 10000 random samples\r\n\r\n![RRT Star Holonomic](https://raw.githubusercontent.com/iamprem/temp/master/assets/rrtstar_holo.gif)\r\n\r\n### Non-Holonomic Robot\r\n\r\n***Yellow Lines near the goal(Red Square) represents the maneuvers performed to reach the goal as desired***\r\n\r\n#### RRT with 1000 random samples \r\n\r\n![RRT Non Holonomic](https://raw.githubusercontent.com/iamprem/temp/master/assets/rrt_nonholo.png)\r\n\r\n#### RRT* with 1000 random samples\r\n\r\nThe below images shows rewiring step(by doing type1 and type2 maneuvers)\r\nin multiple places.\r\n\r\n![RRT Star Non Holonomic](https://raw.githubusercontent.com/iamprem/temp/master/assets/rrtstar_nonholo.png)\r\n\r\n## Execution Instruction\r\n\r\n### Dependencies\r\n \r\n - Python 2.7\r\n - [Pygame](http://www.pygame.org/wiki/CompileUbuntu)\r\n - Numpy\r\n - Ubuntu 14.04 or 15.10\r\n \r\n#### To Install Numpy and Pygame on Ubuntu\r\n\r\n ```bash\r\n sudo apt-get install pip            # Install pip(python package manager)\r\n sudo pip install -U numpy           # Install Numpy using pip\r\n sudo apt-get install python-pygame  # Install Pygame using Ubuntu package manager\r\n ```\r\n \r\n***Note: This program can also be run on Windows and Mac with appropriate pygame [installation](http://www.pygame.org/download.shtml)***\r\n\r\n### Creating and Loading Scene\r\n\r\nTo create scene(2D-environment with obstacles), run the below command from terminal/command prompt.\r\n \r\n ```bash\r\n cd rmp                              # Navigate inside the project folder\r\n python scene.py                     # Run scene definer\r\n ```\r\n\r\nIn the Screen resulted by running the above command, you can draw shapes by doing any of the following.\r\n 1. Click and Drag to draw rectangles (like in MS Paint)\r\n 2. Or Just **click on 3 or more places** on the screen to capture co-ordinates of the polygon and once done press **ENTER** to \r\n see the polygonal shape(Make sure that you don't drag the mouse while clicking).\r\n \r\nOnce obstacles are defined, Close the window to save the scene to 'scene_01.pkl' file(will be located on the project root directory) \r\n\r\n\r\n\r\n### Running Path Planning Methods\r\n\r\nUse ```playground.py``` located in project root to test the program.\r\n\r\n#### To Define Initial and Goal configurations\r\n Change the ```q_init``` and ```q_goal``` at line 11 and 12 on playground.py. ```x,y and theta``` should be in the range ```(0-800), (0-800)\r\n and (-3.14, 3.14)``` respectively.\r\n ```python\r\n playground.py : Line 11-12\r\n    \r\n    q_init = (100.0, 500.0, 0.0)\r\n    q_goal = (700.0, 500.0, -2.15)\r\n ```\r\n\r\n#### Choose Algorithm\r\n\r\nUncomment any one line and change parameters to run the required algorithm.\r\n```python\r\nplayground.py : Line 27-31\r\n   \r\n   # Call algorithm\r\n   # rrt_tree = planner.build_rrt(10000, epsilon=5)\r\n   # rrt_tree = planner.build_rrtstar(K=10000, epsilon=5)\r\n   # rrt_tree = planner.nh_build_rrt(K=1000, epsilon=40)\r\n   # rrt_tree = planner.nh_build_rrtstar(K=1000, epsilon=40)\r\n```\r\n***Warning: Non-Holonomic cases with K > 1000 might take longer time to produce result because of inefficient graph\r\nplotting***\r\n\r\nOnce you uncommented the algorithm in this step, uncomment/comment the required plotting line to see the result.\r\n\r\n\r\n#### Holonomic Plot\r\nFor Holonomic cases, Uncomment the below lines and comment the Lines (51-57) shown in the next step(Non-Holonomic Plot)\r\n\r\n```python\r\nplayground.py : Line 45-47\r\n\r\n   # q_goal_vtx = planner.reach_goal(rrt_tree, q_goal)\r\n   # vizer.plot_graph(rrt_tree, q_init)\r\n   # vizer.trace_path(q_goal_vtx)\r\n```\r\n\r\n\r\n\r\n#### Non-Holonomic Plot\r\nFor Non-Holonomic Case, comment the three lines (45-47) and uncomment lines (51-57)\r\n\r\n```python\r\nplayground.py : Lines 51-57\r\n\r\n   vizer.nh_plot_graph(rrt_tree, q_init)\r\n   vizer.nh_trace_path(rrt_tree.getVertex(q_nearest))\r\n   \r\n   a,b,c,d = planner.nh_reach_goal(q_nearest, q_goal)\r\n   final_list = planner.append_t1_m_t2(a, b, c)\r\n   # TODO final_list collision check\r\n   vizer.plot_points(final_list, vizer.YELLOW, 3)\r\n```\r\n\r\n#### Start and Stop\r\nAfter selecting the appropriate lines from the above steps, \r\n - run ***```python playground.py```*** to start the program.\r\n - Close the window to exit the program once done(Or Press CTRL + C on console to break)\r\n\r\n## Source Code\r\n\r\n[On Github](http://github.com/iamprem/rmp)\r\n\r\n## Future Work\r\n \r\n - Improve Collision detection to more accurate methods and define the car as a polygon instead of point.\r\n - Type 1 and Type 2 maneuvers should be optimized to reduce number of intermediate steps\r\n - Implement bidirectional RRTs\r\n \r\n## References\r\n\r\n 1. Karaman, Sertac, and Emilio Frazzoli. \"Incremental sampling-based algorithms for optimal motion planning.\" arXiv preprint arXiv:1005.0416 (2010).\r\n 2. Karaman, Sertac, and Emilio Frazzoli. \"Sampling-based algorithms for optimal motion planning.\" The International Journal of Robotics Research 30.7 (2011): 846-894.\r\n 3. Karaman, Sertac, and Emilio Frazzoli. \"Optimal kinodynamic motion planning using incremental sampling-based methods.\" Decision and Control (CDC), 2010 49th IEEE Conference on. IEEE, 2010.\r\n 4. Karaman, Sertac, and Emilio Frazzoli. \"Sampling-based optimal motion planning for non-holonomic dynamical systems.\" Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEE, 2013.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}